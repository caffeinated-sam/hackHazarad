import os
import subprocess
import requests
import json
import sqlite3

from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings

  # Updated import to avoid deprecation

from ai.check_pdf import check_pdfs
# from ai.conversation_db import save_conversation_history  # Uncomment if using conversation logging

# === SETTINGS ===
AI_DIR = os.path.join(os.path.dirname(__file__), "ai")
DATA_DIR = os.path.join(os.path.dirname(__file__), "data", "medical-papers")
INDEX_DIR = os.path.join(os.path.dirname(__file__), "embeddings", "faiss_medical_db")
API_KEY_PATH = os.path.join(AI_DIR, "config.py")
MODEL_NAME = "llama3-70b-8192"

# === Load API Key ===
with open(API_KEY_PATH, "r") as key_file:
    GROQ_API_KEY = key_file.read().strip()

# === STEP 1: Vectorization Pipeline ===
def run_vector_pipeline():
    try:
        print("ğŸ“‚ Checking PDF files in the directory...")
        check_pdfs(DATA_DIR)

        print("ğŸ§  Extracting and chunking medical documents...")
        subprocess.run(["python", os.path.join(AI_DIR, "extract_and_chunk.py")], check=True)

        print("ğŸ“ˆ Building FAISS vector index...")
        subprocess.run(["python", os.path.join(AI_DIR, "build_faiss_index.py")], check=True)

        index_file = os.path.join(INDEX_DIR, "index.faiss")
        if os.path.exists(index_file):
            print(f"ğŸ‰ FAISS index created successfully at {index_file}")
        else:
            print("âŒ FAISS index creation failed.")

        return "âœ… Vector pipeline executed successfully."
    except subprocess.CalledProcessError as e:
        return f"âŒ Pipeline failed: {str(e)}"

# === STEP 2: Context Retrieval ===
def get_relevant_context(user_input):
    try:
        if not os.path.exists(os.path.join(INDEX_DIR, "index.faiss")):
            return "âš ï¸ FAISS index not found. Please run the vector pipeline first."

        embedding = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

        print("ğŸ”§ Loading FAISS index...")
        db = FAISS.load_local(INDEX_DIR, embedding, allow_dangerous_deserialization=True)

        print("ğŸ” Performing similarity search...")
        docs = db.similarity_search(user_input, k=3)

        if not docs:
            print("ğŸ“­ No matching documents found.")
            return "No relevant context found."

        return "\n\n".join([doc.page_content for doc in docs])

    except Exception as e:
        print(f"âŒ Error while fetching context: {e}")
        return "âš ï¸ Failed to fetch context."


# === STEP 3: GROQ API Call ===
def get_groq_response(context, user_input, debug=False):
    api_url = "https://api.groq.com/openai/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {GROQ_API_KEY}",
        "Content-Type": "application/json"
    }

    data = {
        "model": MODEL_NAME,
        "messages": [
            {
                "role": "system",
                "content": "You are a helpful medical assistant. Only answer using the context provided."
            },
            {
                "role": "user",
                "content": f"Context:\n{context}\n\nQuestion: {user_input}"
            }
        ],
        "temperature": 0.3
    }

    try:
        if debug:
            print("ğŸ“¡ Sending request to GROQ API...")
            print("Payload:\n", json.dumps(data, indent=2))

        response = requests.post(api_url, headers=headers, json=data)
        response.raise_for_status()
        response_json = response.json()

        if debug:
            print("âœ… Response received:", json.dumps(response_json, indent=2))

        if 'choices' in response_json and response_json['choices']:
            return response_json['choices'][0]['message']['content']
        else:
            return "âš ï¸ GROQ responded, but no answer was found."

    except requests.exceptions.RequestException as e:
        return f"ğŸš« Request to GROQ API failed: {e}"
    except ValueError:
        return f"ğŸš« Received non-JSON response: {response.text}"
    except KeyError as e:
        return f"ğŸš« Missing expected data in response: {e}"

# === CAMERA CONNECT ===
def run_object_detection():
    subprocess.run(["python", "detect_objects.py"])

# === ENTRY POINT ===
if __name__ == "__main__":
    print("ğŸš€ Starting Health AI Vector + GROQ pipeline...")

    

    # Optionally run the pipeline (only needed once)
    # status = run_vector_pipeline()
    # print(status)

    while True:
        user_input = input("\nğŸ’¬ Ask a health question (or type 'exit' to quit): ")
        if user_input.lower() == "exit":
            print("ğŸ‘‹ Exiting chatbot.")
            break

        context = get_relevant_context(user_input)
        reply = get_groq_response(context, user_input)

        print("\nğŸ¤– GROQ Health Bot:")
        print(reply)

        # save_conversation_history("user", user_input)
        # save_conversation_history("assistant", reply)
